{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import Data_Clean_Process as dc\n",
    "import tn_helper as tn\n",
    "# from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.kernel_approximation import Nystroem, RBFSampler\n",
    "# from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Yelp Restaurants as Successful#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp holds contests for students and the public where they release a large amount of data, and then have the particpants compete to reach one of a few specialized goals, ranging from building social network graphs to image recognition. We are using this data to tackle our own question, can we predict whether a restaurant has a good rating (four or five stars) or a bad rating (one, two or three stars). Because of the size of the dataset, we are storing it on a google cloud sql server, so our first step is to fetch only the data that we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing ##\n",
    "Once the data was on our sql server, we selected a subset of restaurants that we wanted to focus on and then looked at all of the reviews for each restaurant. We then computed some descriptive statistics about the review set for each restaurant to use as features in our classification model. The final list of features we used is listed at the end of this section, but some of the things we looked at were the review wordcounts and the engagement the reveiws received from other users. After creating those features on the sql server we then downloaded the data to python to set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_data = dc.download_data(\"K*%4t3VK0ab%gn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data cleaning was just formatting strings and filling in missing values without imputation, we clean before doing our train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data, numerical_features, categorical_features = dc.clean_data(\n",
    "    business_data)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = dc.tt_split(clean_data, numerical_features, categorical_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines ##\n",
    "We set up piplines to use for hyperparameter tuning for all of the models that we used to classify our yelp restaurants. The first step is to make a column transformer that applies the correct preprocessing steps to each feature column in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = make_column_transformer(\n",
    "    (StandardScaler(), numerical_features),\n",
    "    (OneHotEncoder(drop='first'), categorical_features)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the pipelines for the four models we tested, Logistic Regression, Random Forest, and two SVM methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([('ct', preprocess),\n",
    "                    ('clf', LogisticRegression())])\n",
    "\n",
    "grid_params_lr = [{'clf__penalty': ['l1', 'l2'],\n",
    "                   'clf__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "                   }]\n",
    "\n",
    "log_results = pd.read_pickle('./CV_Results/Logistic_CV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([('scl', preprocess),\n",
    "                    ('clf', RandomForestClassifier(oob_score=True, n_jobs=-1, verbose=1))])\n",
    "\n",
    "grid_params_rf = [{'clf__criterion': ['gini'],\n",
    "                   'clf__n_estimators': [300, 500],\n",
    "                   'clf__min_samples_leaf': [10, 13, 15, 17],\n",
    "                   'clf__max_depth': [20, 25, 30],\n",
    "                   'clf__max_features': ['log2', 'sqrt']\n",
    "                   }]\n",
    "\n",
    "rf_results = pd.read_pickle('./CV_Results/Random_Forest_final_CV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM ###\n",
    "Our two svm methods below use two different methods to approximate the radial kernel SVM using a linear SVC. One uses Nystroem method and the other uses a monte carlo method to approximate the radial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm_ny = Pipeline([('ct', preprocess),\n",
    "                        (\"feature_map\", Nystroem(gamma=.2)),\n",
    "                        ('clf', LinearSVC(verbose=1, max_iter=15000, dual=False))])\n",
    "\n",
    "grid_params_svm_ny = [{'feature_map__gamma': [.005, .01, .1],\n",
    "                       'clf__C': [3000, 5000, 7000],\n",
    "                       'clf__penalty': ['l1']\n",
    "                       }]\n",
    "\n",
    "svm_ny_results = pd.read_pickle('./CV_Results/SVM_Ny_RBF_CV')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svm_mc = Pipeline([('ct', preprocess),\n",
    "                        (\"feature_map\", RBFSampler()),\n",
    "                        ('clf', LinearSVC(verbose=1, max_iter=15000, dual=False))])\n",
    "\n",
    "grid_params_svm_mc = [{'feature_map__gamma': [.005, .01, .02],\n",
    "                       'clf__C': [1000, 2000, 3000],\n",
    "                       'clf__penalty': ['l1']\n",
    "                       }]\n",
    "\n",
    "svm_mc_results = pd.read_pickle('./CV_Results/SVM_MC_RBF_3_CV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter Tuning ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This executes the grid search over all the choices of parameters that have been specified in the pipeline. To run the search, set estimator to the pipline you want, set param_grid to the corresponding param grid dictionary, set cv to the number of crossvalidation steps, and set scoring equal to the scoring metric you would like to use in crossvalidation.Depending on the choice of model and hyperparameters this grid search may take a very long time to run, so we have left it commented out. Below we have summaries of the grid search results for each model. The full results of the grid searches are saved as pickled dataframes in the CV_Results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch = GridSearchCV(estimator=pipe,\n",
    "#                           param_grid=grid_params_rf,\n",
    "#                           scoring='accuracy',\n",
    "#                           return_train_score=True,\n",
    "#                           cv=5)\n",
    "\n",
    "# gridsearch.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a list of how the tuned versions of our models performed, with the list of hyper parameters, the mean cv test score and the mean fit time for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[rf_results, svm_mc_results, svm_ny_results, log_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=150\n",
    "tn.top_models(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Random Forest model performed the best, but after tuning all of the models performed similarly in terms of accuracy. We decided to use Random forest as our model not only because it had the best performance, but because it had the second fastest mean fit time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(\n",
    "    max_depth=30, max_features='sqrt', min_samples_leaf=10, n_estimators=300, n_jobs=-1,oob_score=True)\n",
    "\n",
    "t_X_train=preprocess.fit_transform(X_train)\n",
    "t_X_test=preprocess.fit_transform(X_test)\n",
    "\n",
    "random_forest.fit(t_X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our final model on all of the testing data, and then verify it against the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training accuracy\n",
    "random_forest.score(t_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy\n",
    "random_forest.score(t_X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=ConfusionMatrix(random_forest)\n",
    "cm.poof()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test score for our model on data that has not been used for hyperparameter tuning agrees with our crossvalidation results, so we can be pretty confident in the ability of our model to classify unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances=random_forest.feature_importances_\n",
    "# ohe=OneHotEncoder(drop='first').fit(X_test[categorical_features])\n",
    "# categorical_names=ohe.get_feature_names()\n",
    "# feature_names=numerical_features+list(categorical_names)\n",
    "# importances_df=pd.concat([pd.DataFrame(importances),pd.DataFrame(feature_names)],axis=1)\n",
    "# importances_df.columns=['Importance','Feature']\n",
    "# importances_df.sort_values(by='Importance',ascending=False).head(10)\n",
    "importances_df=feature_importance(random_forest, X_train, categorical_features)\n",
    "importances_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar(x=importances_df.Feature, height=importances_df.Importance)\n",
    "plt.savefig('features.png',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features[22],categorical_features[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
